{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rostenbach/Notebooks/blob/Notebooks/mlc-llm/tutorial_chat_module_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm85Ap3zDmYB"
      },
      "source": [
        "# Getting Started with MLC-LLM using the Llama 2 Model\n",
        "\n",
        "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ttPt-hNDmYC"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Let's set up your environment, so you can successfully run the `ChatModule`. First, let's set up the Conda environment which we will be running this notebook in (not required if running in Google Colab).\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```\n",
        "\n",
        "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
        "\n",
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KK25HZsIDmYC",
        "outputId": "fbfbe361-2fb6-4a32-b6cf-a194f25b9ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 15 23:35:23 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOtpjJMDmYE"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PgW-5OAADmYE",
        "outputId": "5eb2346d-83cf-45e7-b7ca-7888eaa41ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1574-cp310-cp310-manylinux_2_28_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev423-cp310-cp310-manylinux_2_28_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Downloading numpy-1.26.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Downloading typing_extensions-4.8.0rc1-py3-none-any.whl (31 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna>=2.8 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1 (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.3 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.2\n",
            "    Uninstalling tornado-6.3.2:\n",
            "      Successfully uninstalled tornado-6.3.2\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.3\n",
            "    Uninstalling exceptiongroup-1.1.3:\n",
            "      Successfully uninstalled exceptiongroup-1.1.3\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.2\n",
            "    Uninstalling scipy-1.11.2:\n",
            "      Successfully uninstalled scipy-1.11.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.12\n",
            "    Uninstalling pydantic-1.10.12:\n",
            "      Successfully uninstalled pydantic-1.10.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.0rc1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0rc1 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0rc1 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0rc1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.5.0 anyio-3.7.1 attrs-23.1.0 click-8.1.7 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.3 fastapi-0.103.1 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu118-0.12.dev1574 mlc-chat-nightly-cu118-0.1.dev423 numpy-1.26.0rc1 psutil-5.9.5 pydantic-2.3.0 pydantic-core-2.6.3 scipy-1.11.2 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.3 typing-extensions-4.8.0rc1 uvicorn-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn7MYEFt5tvY"
      },
      "source": [
        "**Google Colab:** If in Google Colab, you may see a message warning you to restart the runtime. Simply run the following code in a new code cell to restart the runtime.\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwsWd1WbDmYE"
      },
      "source": [
        "Next, let's download the model weights for the Llama2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvAhErV3gjq"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell to fully install `git lfs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0GjINnMDmYF",
        "outputId": "a7e053a0-81da-4bfc-b11d-88ef6f212576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYwjsCOK7Jij"
      },
      "source": [
        "These commands will download many prebuilt libraries as well as the chat configuration for Llama-2-7b that `mlc_chat` needs, which may take a long time. If in **Google Colab** you can verify that the files are being downloaded by clicking on the folder icon on the left and navigating to the `dist` and then `prebuilt` folders which should be updating as the files are being downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FSAe7Ew_DmYF",
        "outputId": "59c5649a-e7de-4c0f-fc10-8dc79e6d5bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dist/prebuilt/lib'...\n",
            "remote: Enumerating objects: 290, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 290 (delta 61), reused 49 (delta 47), pack-reused 217\u001b[K\n",
            "Receiving objects: 100% (290/290), 67.14 MiB | 13.93 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n",
            "Updating files: 100% (92/92), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BDbi6H3MDmYF",
        "outputId": "97759375-3224-46ae-b01f-612cf2e0fa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 129 (delta 0), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (129/129), 500.53 KiB | 19.25 MiB/s, done.\n",
            "Filtering content: 100% (116/116), 3.53 GiB | 68.98 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ru5__tDmYF"
      },
      "source": [
        "## Let's Chat!\n",
        "\n",
        "Before we can chat with the model, we must first import a library and instantiate a `ChatModule` instance. The `ChatModule` must be initialized with the appropriate model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AJAt6oW7DmYF"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout\n",
        "\n",
        "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9m5sxyXDmYF"
      },
      "source": [
        "Note that the above invocation abstracts away the logic for finding the relevant model directory and prebuilt library paths. To specify these manually, you could run the following instead (which would be equivalent to the above).\n",
        "\n",
        "```python\n",
        "cm = ChatModule(model=\"dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\", lib_path=\"dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEaVXnnJDmYF"
      },
      "source": [
        "That is all what needed to set up the `ChatModule`. You can now chat with the model by entering any prompt you'd like. Try it out below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TNmg9N_NDmYF",
        "outputId": "2334c59f-a1cc-488f-94b7-e2f58844c701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great, thank you for asking! Python was first released in 1991 by Guido van Rossum. He started working on Python in December 1989 while he was still working at the National Research Institute for Mathematics and Computer Science in the Netherlands. The first publicly available version of Python, version 0.9.1, was released on February 20, 1991. Since then, Python has grown to become one of the most popular programming languages in the world, known for its simplicity, readability, and versatility.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"When was Python released?\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ5Nb05tMlJ_"
      },
      "source": [
        "You can also repeat running the code block below for multiple rounds to interact with the model in a chat style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uKvhXgRRMlJ_",
        "outputId": "dfdcb420-f3f0-4402-9fcf-178d9e4abac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: I am a novelist and I am writing a deep expensive series of books that will include some very immersive world building. This is intended to be a continuation of the works in the Lovecraft mythos and partially set within the dreamland. I need you to act as a researcher and assistant. Tell me everything you can about The council of elders\n",
            "Of course, I'm happy to help! The Council of Elders is a fascinating aspect of the Lovecraftian mythos, and I'll do my best to provide you with as much information as possible.\n",
            "\n",
            "The Council of Elders is a group of ancient, powerful beings who are said to reside in the Dreamland, a realm that exists parallel to our own. According to Lovecraftian lore, the Council is composed of various elder gods, including Azathoth, Yog-Sothoth, and Nyarlathotep, among others. These deities are believed to have created the Dreamland as a refuge for themselves, a place where they could escape the mundane world and indulge in their dark, twisted desires.\n",
            "\n",
            "The Council of Elders is said to be a collective of these elder gods, who work together to maintain the balance of power within the Dreamland. They are believed to be incredibly ancient, with some sources suggesting that they have existed for billions of years. Despite their immense power, however, the Council is not above criticism, and some have accused them of being manipulative, cruel, and even tyrannical.\n",
            "\n",
            "One of the most intriguing aspects of the Council of Elders is their ability to manipulate the fabric of reality itself. According to Lovecraftian lore, the elder gods possess the power to warp time, space, and matter, bending reality to their will. This allows them to create entire realms within the Dreamland, each with its own unique properties and inhabitants.\n",
            "\n",
            "The Council of Elders is also said to be obsessed with knowledge, and they are believed to possess a vast library of forbidden lore. This library contains secrets and forbidden truths that are too terrible for mortals to comprehend, and it is said that anyone who dares to access this knowledge will be driven mad by the sheer audacity of it.\n",
            "\n",
            "In addition to their manipulation of reality and their vast knowledge, the Council of Elders is also said to possess incredible magical abilities. They are believed to be able to perform feats of magic that would be impossible for mortals, such as summoning entire armies of the damned or conjuring up entire realms with a mere thought.\n",
            "\n",
            "Despite their incredible power, however, the Council of Elders is\n"
          ]
        }
      ],
      "source": [
        "prompt = input(\"Prompt: \")\n",
        "output = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dqJH2gLeMlJ_",
        "outputId": "159dc491-8453-4a8e-d402-f14c7e26f9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here's a summary of my response:\n",
            "\n",
            "The Council of Elders is a group of ancient, powerful beings in the Lovecraftian mythos who reside in the Dreamland. They are believed to manipulate reality, possess vast knowledge, and possess incredible magical abilities. Despite their immense power, the Council is not above criticism, and some have accused them of being manipulative, cruel, and even tyrannical.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"Please summarize your response in three sentences.\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4bOyUk7DmYF"
      },
      "source": [
        "To check the generation speed of the chat bot, you can print the statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PPbPj6vpDmYF",
        "outputId": "77251f3a-aff1-460d-f395-41642187e50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefill: 173.9 tok/s, decode: 31.0 tok/s\n"
          ]
        }
      ],
      "source": [
        "print(cm.stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAb-XZPnDmYF"
      },
      "source": [
        "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iKpKgVxNDmYF",
        "outputId": "553e85c1-d098-4152-be8c-5951a96eba1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great! Here's a summary of the Council of Elders in three sentences:\n",
            "\n",
            "The Council of Elders is a group of powerful beings in the Lovecraftian mythos who reside in the Dreamland. They manipulate reality, possess vast knowledge, and possess incredible magical abilities. Despite their immense power, the Council is not above criticism, with some accusing them of being manipulative, cruel, and even tyrannical.\n"
          ]
        }
      ],
      "source": [
        "#cm.reset_chat()\n",
        "print(cm.generate(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht4RYKPhMlKA"
      },
      "source": [
        "### Benchmark Performance\n",
        "\n",
        "To benchmark the performance, we can use the `benchmark_generate` method of ChatModule. It takes an input prompt and the number of tokens to generate, ignores the system prompt and model stop criterion, generates tokens in a language model way and stops until finishing generating the desired number of tokens. After calling `benchmark_generate`, we can use `stats` to check the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J60oFJdLMlKA",
        "outputId": "ebbfa5d9-9a6b-4c99-8035-ac4feb12ea9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What is the difference between a benchmark and a reference?  What is the difference between a benchmark and a standard?  What is the difference between a benchmark and a yardstick?  What is the difference between a benchmark and a touchstone?  What is the difference between a benchmark and a marker?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of comparison?  What is the difference between a benchmark and a reference point?  What is the difference between a benchmark and a point of reference?  What is the difference between a benchmark and a reference point?  What is\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prefill: 43.1 tok/s, decode: 40.0 tok/s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "print(cm.benchmark_generate(prompt=\"What is benchmark?\", generate_length=512))\n",
        "cm.stats()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}